from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext

conf = SparkConf().setAppName("Word-Count") # only for pyspark-shell
sc = SparkContext(conf=conf) # only for pyspark-shell
baseRdd = sc.textFile('wasbs://ghtestbigdata@ghtestdatastacc.blob.core.windows.net/HOL-Data/Lab01/selfishgiant.txt')

splitRdd02 = baseRdd.flatMap(lambda line: line.split(" "))
mappedRdd = splitRdd02.map(lambda line: (line,1))

reducedRdd = mappedRdd.reduceByKey(lambda a,b: a+b)
reducedRdd.show()
